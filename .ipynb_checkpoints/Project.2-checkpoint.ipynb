{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Harrison\n",
      "[nltk_data]     Lu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>mon apr 06 22:19:45 pdt 2009</td>\n",
       "      <td>no_query</td>\n",
       "      <td>_thespecialone_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>mon apr 06 22:19:49 pdt 2009</td>\n",
       "      <td>no_query</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mon apr 06 22:19:53 pdt 2009</td>\n",
       "      <td>no_query</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@kenichan i dived many times for the ball. man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>mon apr 06 22:19:57 pdt 2009</td>\n",
       "      <td>no_query</td>\n",
       "      <td>ellectf</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>mon apr 06 22:19:57 pdt 2009</td>\n",
       "      <td>no_query</td>\n",
       "      <td>karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          ID                          date     query  \\\n",
       "0          0  1467810369  mon apr 06 22:19:45 pdt 2009  no_query   \n",
       "1          0  1467810672  mon apr 06 22:19:49 pdt 2009  no_query   \n",
       "2          0  1467810917  mon apr 06 22:19:53 pdt 2009  no_query   \n",
       "3          0  1467811184  mon apr 06 22:19:57 pdt 2009  no_query   \n",
       "4          0  1467811193  mon apr 06 22:19:57 pdt 2009  no_query   \n",
       "\n",
       "          username                                               text  \n",
       "0  _thespecialone_  @switchfoot http://twitpic.com/2y1zl - awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his facebook by ...  \n",
       "2         mattycus  @kenichan i dived many times for the ball. man...  \n",
       "3          ellectf    my whole body feels itchy and like its on fire   \n",
       "4           karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./dataset/sentiment140/data.csv', encoding='latin-1', header = None)\n",
    "df.columns = ['sentiment', 'ID', 'date', 'query', 'username', 'text']\n",
    "\n",
    "df = df.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfTrain = df.sample(frac = 0.8, random_state = 20)\n",
    "dfTest = df.drop(dfTrain.index)\n",
    "\n",
    "dfGraphModel = dfTrain.sample(frac = 0.5, random_state = 20)\n",
    "dfDM = dfTrain.drop(dfGraphModel.index)\n",
    "\n",
    "#subset of the dfTrain dataset, split into positive, negative, and neutral tweets\n",
    "pos = dfGraphModel.loc[df['sentiment'] == 4]\n",
    "neg = dfGraphModel.loc[df['sentiment'] == 0]\n",
    "neut = dfGraphModel.loc[df['sentiment'] == 2]\n",
    "\n",
    "posText = {line[\"ID\"]: line[\"text\"] for index, line in pos.iterrows()}\n",
    "negText = {line[\"ID\"]: line[\"text\"] for index, line in neg.iterrows()}\n",
    "\n",
    "posWords = [line.rstrip('\\n') for line in open('./dataset/positive-words.txt') if line.split() and list(line)[0] != ';']\n",
    "negWords = [line.rstrip('\\n') for line in open('./dataset/negative-words.txt') if line.split() and list(line)[0] != \";\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1598507</th>\n",
       "      <td>4</td>\n",
       "      <td>2193188422</td>\n",
       "      <td>tue jun 16 08:06:59 pdt 2009</td>\n",
       "      <td>no_query</td>\n",
       "      <td>melisarenea</td>\n",
       "      <td>ughh hate wake early drink...aren't supposed s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598732</th>\n",
       "      <td>4</td>\n",
       "      <td>2193255029</td>\n",
       "      <td>tue jun 16 08:12:27 pdt 2009</td>\n",
       "      <td>no_query</td>\n",
       "      <td>jessie2point0</td>\n",
       "      <td>needs food iphone?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599079</th>\n",
       "      <td>4</td>\n",
       "      <td>2193344265</td>\n",
       "      <td>tue jun 16 08:19:50 pdt 2009</td>\n",
       "      <td>no_query</td>\n",
       "      <td>sanityknit</td>\n",
       "      <td>text tell things going! miss guys!! fun - nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599611</th>\n",
       "      <td>4</td>\n",
       "      <td>2193478364</td>\n",
       "      <td>tue jun 16 08:30:46 pdt 2009</td>\n",
       "      <td>no_query</td>\n",
       "      <td>kirstylol</td>\n",
       "      <td>needs food soo much. going watch t.v shower la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599831</th>\n",
       "      <td>4</td>\n",
       "      <td>2193551690</td>\n",
       "      <td>tue jun 16 08:36:43 pdt 2009</td>\n",
       "      <td>no_query</td>\n",
       "      <td>exbp_buddhist</td>\n",
       "      <td>could born emporer penguin. minus 200, carryin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date     query  \\\n",
       "1598507          4  2193188422  tue jun 16 08:06:59 pdt 2009  no_query   \n",
       "1598732          4  2193255029  tue jun 16 08:12:27 pdt 2009  no_query   \n",
       "1599079          4  2193344265  tue jun 16 08:19:50 pdt 2009  no_query   \n",
       "1599611          4  2193478364  tue jun 16 08:30:46 pdt 2009  no_query   \n",
       "1599831          4  2193551690  tue jun 16 08:36:43 pdt 2009  no_query   \n",
       "\n",
       "              username                                              tweet  \n",
       "1598507    melisarenea  ughh hate wake early drink...aren't supposed s...  \n",
       "1598732  jessie2point0                                 needs food iphone?  \n",
       "1599079     sanityknit  text tell things going! miss guys!! fun - nice...  \n",
       "1599611      kirstylol  needs food soo much. going watch t.v shower la...  \n",
       "1599831  exbp_buddhist  could born emporer penguin. minus 200, carryin...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjectTweetsDict = {} # key = index in original df and value = tweet info (sentiment, id, date, query, username, full tweet)\n",
    "filteredTweets = {} # key = index in original df and value = filtered tweet\n",
    "count = 0\n",
    "for row in df.itertuples():\n",
    "    if 'food' in row[6]:\n",
    "        subjectTweetsDict[row[0]] = list(row)[1:7]\n",
    "        word_tokens = str(row[6]).split() #split by white space\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "        filtered_sentence = [] \n",
    "        i = 0\n",
    "        while i < len(word_tokens):\n",
    "            if '@' in word_tokens[i]: #Taking out handles from tweets\n",
    "                i = i + 1\n",
    "            elif word_tokens[i] not in stop_words:\n",
    "                filtered_sentence.append(word_tokens[i])\n",
    "            i = i + 1\n",
    "        filteredTweets[row[0]] = filtered_sentence\n",
    "#print(subjectTweetsDict)\n",
    "#df2 = pd.DataFrame(data=subjectTweetsDict, columns = ['index', 'sentiment', 'id', 'date', 'query', 'username', 'tweet'])\n",
    "dfSubject= pd.DataFrame.from_dict(subjectTweetsDict, orient = 'index', columns = ['sentiment', 'id', 'date', 'query', 'username', 'tweet'])\n",
    "for key, value in filteredTweets.items():\n",
    "    filteredString = ' '.join(value)\n",
    "    dfSubject.at[key, \"tweet\"] = filteredString\n",
    "\n",
    "dfSubject.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTrain = dfSubject.sample(frac = 0.8, random_state = 20)\n",
    "dfTest = dfSubject.drop(dfTrain.index)\n",
    "\n",
    "dfGraphModel = dfTrain.sample(frac = 0.6, random_state = 20)\n",
    "dfDM = dfTrain.drop(dfGraphModel.index)\n",
    "\n",
    "posSubject = dfGraphModel.loc[dfSubject['sentiment'] == 4]\n",
    "negSubject = dfGraphModel.loc[dfSubject['sentiment'] == 0]\n",
    "neutSubject = dfGraphModel.loc[dfSubject['sentiment'] == 2] #there is no neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createGraph(text, wordGraph, frame):\n",
    "    if text:\n",
    "        text = re.sub(r'[^\\w\\s]', '', str(text))\n",
    "        text = text.split()\n",
    "        try:\n",
    "            for x in range(len(text) - frame):\n",
    "                for y in range(1, frame + 1):\n",
    "                    wordGraph.add_edge(text[x], text[x+y])\n",
    "            for x in reversed(range(1, frame + 1)):\n",
    "                for y in reversed(range(1, x)):\n",
    "                    wordGraph.add_edge(text[len(text) - x], text[len(text) - y])\n",
    "        except IndexError:\n",
    "            return createGraph(text, wordGraph, frame-1)\n",
    "        return wordGraph\n",
    "    else:\n",
    "        return wordGraph\n",
    "    \n",
    "def createGraphFromTweet(text, frame):\n",
    "    wordGraph = nx.DiGraph()\n",
    "    if type(text) == str:\n",
    "        createGraph(text, wordGraph, frame)\n",
    "    if type(text) == list:\n",
    "        for element in text:\n",
    "            createGraph(element, wordGraph, frame)\n",
    "    return wordGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harrison Lu\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Harrison Lu\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "posArrayText = posSubject.as_matrix(columns = posSubject.columns[-1:]).flatten().tolist()\n",
    "negArrayText = negSubject.as_matrix(columns = negSubject.columns[-1:]).flatten().tolist()\n",
    "posWordGraph = createGraphFromTweet(posArrayText, 4)\n",
    "negWordGraph = createGraphFromTweet(negArrayText, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#graph similary functions\n",
    "\n",
    "def edgeSimilarity(inputGraph, model):\n",
    "    count = 0\n",
    "    for edge in inputGraph.edges():\n",
    "        n1, n2 = edge\n",
    "        if model.has_edge(n1, n2): \n",
    "            count += 1\n",
    "    return count/min(len(inputGraph), len(model))\n",
    "\n",
    "def getMCS(graphModel, tweetGraph):\n",
    "    matching_graph=nx.Graph() #subgraph\n",
    "\n",
    "    for n1,n2,attr in tweetGraph.edges(data=True):\n",
    "        if graphModel.has_edge(n1,n2) :\n",
    "            matching_graph.add_edge(n1,n2,weight=1)\n",
    "\n",
    "    graphs = list(nx.connected_component_subgraphs(matching_graph))\n",
    "\n",
    "    mcs_length = 0\n",
    "    mcs_graph = nx.DiGraph() \n",
    "    \n",
    "    for i, graph in enumerate(graphs):                        #Finding maximum subgraph out of all graphs\n",
    "\n",
    "        if len(graph.nodes()) > mcs_length: \n",
    "            mcs_length = len(graph.nodes())\n",
    "            mcs_graph = graph\n",
    "\n",
    "    return mcs_graph\n",
    "\n",
    "def MCSNS(mcs_graph, graphModel, tweetGraph): #number of nodes in common subgraph divided by minimum number of nodes\n",
    "    return len(mcs_graph)/min(len(graphModel),len(tweetGraph))\n",
    "\n",
    "def MCSUES(mcs_graph, graphModel, tweetGraph): #number of edges in MCS divided by min number of nodes\n",
    "    return len(mcs_graph.edges())/min(len(graphModel),len(tweetGraph))\n",
    "\n",
    "def MCSDES(mcs_graph, graphModel, tweetGraph): #edges in the mcs_graph are the same direction in both graphs\n",
    "    count = 0\n",
    "    for e1,e2 in mcs_graph.edges():\n",
    "        if tweetGraph.has_edge(e1,e2) and graphModel.has_edge(e1,e2):\n",
    "            count+=1\n",
    "    return count/min(len(graphModel),len(tweetGraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentimentTweetDict = dfDM.to_dict(orient='index')\n",
    "\n",
    "def generateWordGraphVectors(dataframeDict, posWordGraph, negWordGraph, metricType):\n",
    "    X = []\n",
    "    y = []\n",
    "    count = 0\n",
    "    for key, value in dataframeDict.items():\n",
    "        if value['tweet']:\n",
    "            y.append(value['sentiment'])\n",
    "            wordGraph = createGraphFromTweet(value['tweet'], 4)\n",
    "            posNegArray = []\n",
    "            if metricType == \"edge\":\n",
    "                posNegArray.append(edgeSimilarity(wordGraph, posWordGraph))\n",
    "                posNegArray.append(edgeSimilarity(wordGraph, negWordGraph))\n",
    "            elif metricType == \"MCSNS\":\n",
    "                mcs_graph = getMCS(posWordGraph, wordGraph)\n",
    "                posNegArray.append(MCSNS(mcs_graph, wordGraph, posWordGraph))\n",
    "                mcs_graph = getMCS(negWordGraph, wordGraph)\n",
    "                posNegArray.append(MCSNS(mcs_graph, wordGraph, negWordGraph))\n",
    "            elif metricType == \"MCSUES\":\n",
    "                mcs_graph = getMCS(posWordGraph, wordGraph)\n",
    "                posNegArray.append(MCSUES(mcs_graph, wordGraph, posWordGraph))\n",
    "                mcs_graph = getMCS(negWordGraph, wordGraph)\n",
    "                posNegArray.append(MCSUES(mcs_graph, wordGraph, negWordGraph))\n",
    "            elif metricType == \"MCSDES\":\n",
    "                mcs_graph = getMCS(posWordGraph, wordGraph)\n",
    "                posNegArray.append(MCSDES(mcs_graph, wordGraph, posWordGraph))\n",
    "                mcs_graph = getMCS(negWordGraph, wordGraph)\n",
    "                posNegArray.append(MCSDES(mcs_graph, wordGraph, negWordGraph))\n",
    "            X.append(posNegArray)\n",
    "            count += 1\n",
    "    return X, y\n",
    "\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraph, negWordGraph, \"edge\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraph, negWordGraph, \"edge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.582345971563981"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try kNN method\n",
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "model.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.620260663507109"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#svm\n",
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "classifier.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#linear regression\n",
    "from sklearn.linear_model import LogisticRegression as logreg\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "classifier.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5598341232227488"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decision tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier as dectree\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "classifier.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.5438388625592417\n",
      "SVM score: 0.580568720379147\n",
      "Logistic regression score: 0.5817535545023697\n",
      "Decision tree score: 0.5545023696682464\n"
     ]
    }
   ],
   "source": [
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraph, negWordGraph, \"MCSNS\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraph, negWordGraph, \"MCSNS\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.5989336492890995\n",
      "SVM score: 0.6149289099526066\n",
      "Logistic regression score: 0.6137440758293838\n",
      "Decision tree score: 0.5610189573459715\n"
     ]
    }
   ],
   "source": [
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraph, negWordGraph, \"MCSUES\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraph, negWordGraph, \"MCSUES\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.5361374407582938\n",
      "SVM score: 0.5669431279620853\n",
      "Logistic regression score: 0.5728672985781991\n",
      "Decision tree score: 0.5219194312796208\n"
     ]
    }
   ],
   "source": [
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraph, negWordGraph, \"MCSDES\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraph, negWordGraph, \"MCSDES\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trimming graphs based on a hard limit- how many times a word was used. basically, edge weight of 1 or 0\n",
    "\n",
    "def createTrimmedGraph(text, wordGraph, frame):\n",
    "    if text:\n",
    "        text = re.sub(r'[^\\w\\s]', '', str(text))\n",
    "        text = text.split()\n",
    "        try:\n",
    "            if len(text) == 1:\n",
    "                wordGraph.add_edge(text[0], text[0], weight = 1)\n",
    "            for x in range(len(text) - frame):\n",
    "                for y in range(1, frame + 1):\n",
    "                    n1 = text[x]\n",
    "                    n2 = text[x+y]\n",
    "                    if wordGraph.has_edge(n1, n2):\n",
    "                        wordGraph[n1][n2]['weight'] = wordGraph[n1][n2]['weight'] + 1\n",
    "                    else:\n",
    "                        wordGraph.add_edge(n1, n2, weight = 1)\n",
    "            for x in reversed(range(1, frame + 1)):\n",
    "                for y in reversed(range(1, x)):\n",
    "                    n1 = text[len(text) - x]\n",
    "                    n2 = text[len(text) - y]\n",
    "                    if wordGraph.has_edge(n1, n2):\n",
    "                        wordGraph[n1][n2]['weight'] = wordGraph[n1][n2]['weight'] + 1\n",
    "                    else:\n",
    "                        wordGraph.add_edge(n1, n2, weight = 1)\n",
    "        except IndexError:\n",
    "            return createGraph(text, wordGraph, frame-1)\n",
    "        return wordGraph\n",
    "    else:\n",
    "        return wordGraph\n",
    "    \n",
    "def createGraphWithWeightsFromTweet(text, frame):\n",
    "    wordGraph = nx.DiGraph()\n",
    "    if type(text) == str:\n",
    "        createTrimmedGraph(text, wordGraph, frame)\n",
    "    if type(text) == list:\n",
    "        for element in text:\n",
    "            createTrimmedGraph(element, wordGraph, frame)\n",
    "    return wordGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posWordGraph = createGraphWithWeightsFromTweet(posArrayText, 4)\n",
    "negWordGraph = createGraphWithWeightsFromTweet(negArrayText, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeEdgesByWeight(graph, threshold):\n",
    "    returnGraph = graph.copy()\n",
    "    edgeCountDict = nx.get_edge_attributes(returnGraph, 'weight')\n",
    "    for key, value in edgeCountDict.items():\n",
    "        if value <= threshold:\n",
    "            returnGraph.remove_edge(*key)\n",
    "    emptyNodes = list(nx.isolates(returnGraph))\n",
    "    returnGraph.remove_nodes_from(emptyNodes)\n",
    "    return returnGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('ive', 'scones'): 1,\n",
       " ('ive', 'toothey'): 1,\n",
       " ('ive', 'help'): 1,\n",
       " ('ive', 'lose'): 1,\n",
       " ('ive', 'gotta'): 1,\n",
       " ('ive', 'tell'): 1,\n",
       " ('ive', 'something'): 1,\n",
       " ('ive', 'got'): 1,\n",
       " ('ive', 'headache'): 1,\n",
       " ('ive', 'might'): 1,\n",
       " ('ive', 'go'): 1,\n",
       " ('ive', 'waitin'): 1,\n",
       " ('ive', '4'): 1,\n",
       " ('ive', '4ever'): 1,\n",
       " ('ive', 'started'): 1,\n",
       " ('ive', 'cooking'): 1,\n",
       " ('ive', 'family'): 1,\n",
       " ('ive', 'ministry'): 1,\n",
       " ('ive', 'tried'): 3,\n",
       " ('ive', 'foriegn'): 1,\n",
       " ('ive', 'food'): 5,\n",
       " ('ive', 'craving'): 2,\n",
       " ('ive', 'sweet'): 1,\n",
       " ('ive', 'potatoes'): 1,\n",
       " ('ive', 'almonds'): 1,\n",
       " ('ive', 'still'): 1,\n",
       " ('ive', 'sleep'): 1,\n",
       " ('ive', '40'): 1,\n",
       " ('ive', 'broken'): 1,\n",
       " ('ive', 'found'): 1,\n",
       " ('ive', 'seat'): 1,\n",
       " ('ive', 'bbq'): 1,\n",
       " ('ive', 'right'): 1,\n",
       " ('ive', 'decided'): 1,\n",
       " ('ive', 'gone'): 1,\n",
       " ('ive', '2'): 1,\n",
       " ('ive', 'culinary'): 1,\n",
       " ('ive', 'chopped'): 1,\n",
       " ('ive', 'loooooads'): 1,\n",
       " ('ive', 'greens'): 1,\n",
       " ('ive', 'figure'): 1,\n",
       " ('ive', 'never'): 3,\n",
       " ('ive', 'well'): 1,\n",
       " ('ive', 'see'): 1,\n",
       " ('ive', 'eating'): 1,\n",
       " ('ive', 'copious'): 1,\n",
       " ('ive', 'amounts'): 1,\n",
       " ('ive', 'foods'): 3,\n",
       " ('ive', 'locked'): 1,\n",
       " ('ive', 'door'): 1,\n",
       " ('ive', 'im'): 2,\n",
       " ('ive', 'leaving'): 2,\n",
       " ('ive', 'also'): 1,\n",
       " ('ive', 'contributed'): 1,\n",
       " ('ive', 'discussion'): 1,\n",
       " ('ive', 'previous'): 1,\n",
       " ('ive', 'diet'): 1,\n",
       " ('ive', '6'): 1,\n",
       " ('ive', 'days'): 1,\n",
       " ('ive', 'already'): 1,\n",
       " ('ive', 'washed'): 1,\n",
       " ('ive', 'dishes'): 1,\n",
       " ('ive', 'put'): 1,\n",
       " ('ive', 'since'): 1,\n",
       " ('ive', 'chi'): 1,\n",
       " ('ive', 'posting'): 1,\n",
       " ('ive', 'much'): 1,\n",
       " ('ive', 'today'): 2,\n",
       " ('ive', 'watching'): 1,\n",
       " ('ive', 'lot'): 1,\n",
       " ('ive', 'network'): 1,\n",
       " ('ive', 'yet'): 1,\n",
       " ('ive', 'italy'): 1,\n",
       " ('ive', 'like'): 1,\n",
       " ('ive', 'discovered'): 1,\n",
       " ('ive', 'favorite'): 1,\n",
       " ('ive', 'stuff'): 1,\n",
       " ('ive', 'way'): 1,\n",
       " ('ive', 'moved'): 1,\n",
       " ('ive', 'lunchtime'): 1,\n",
       " ('ive', 'popcorn'): 1,\n",
       " ('ive', 'taken'): 1,\n",
       " ('ive', 'day'): 1,\n",
       " ('ive', 'sit'): 1,\n",
       " ('ive', 'emerged'): 1,\n",
       " ('ive', 'coma'): 1,\n",
       " ('ive', 'israel'): 1,\n",
       " ('ive', 'stayed'): 1,\n",
       " ('ive', 'tel'): 1,\n",
       " ('ive', 'aviv'): 1,\n",
       " ('scones', 'toothey'): 1,\n",
       " ('scones', 'help'): 1,\n",
       " ('scones', 'lose'): 1,\n",
       " ('scones', 'weight'): 1,\n",
       " ('toothey', 'help'): 1,\n",
       " ('toothey', 'lose'): 1,\n",
       " ('toothey', 'weight'): 1,\n",
       " ('toothey', 'make'): 1,\n",
       " ('help', 'lose'): 1,\n",
       " ('help', 'weight'): 1,\n",
       " ('help', 'make'): 1,\n",
       " ('help', 'daily'): 1,\n",
       " ('help', 'hangover'): 1,\n",
       " ('help', 'lol'): 1,\n",
       " ('help', 'all'): 1,\n",
       " ('help', 'move'): 1,\n",
       " ('help', 'now'): 1,\n",
       " ('lose', 'weight'): 2,\n",
       " ('lose', 'make'): 1,\n",
       " ('lose', 'daily'): 1,\n",
       " ('lose', 'food'): 1,\n",
       " ('lose', 'haha'): 1,\n",
       " ('lose', 'still'): 1,\n",
       " ('lose', 'consumed'): 1,\n",
       " ('weight', 'make'): 1,\n",
       " ('weight', 'daily'): 1,\n",
       " ('weight', 'food'): 1,\n",
       " ('weight', 'group'): 1,\n",
       " ('weight', 'loss'): 1,\n",
       " ('weight', 'increasing'): 1,\n",
       " ('weight', 'wondering'): 1,\n",
       " ('weight', 'foods'): 1,\n",
       " ('weight', 'going'): 1,\n",
       " ('weight', 'haha'): 1,\n",
       " ('weight', 'still'): 1,\n",
       " ('weight', 'consumed'): 1,\n",
       " ('weight', 'amount'): 1,\n",
       " ('weight', 'too'): 1,\n",
       " ('weight', 'quothealthy'): 1,\n",
       " ('weight', 'fast'): 1,\n",
       " ('weight', 'foodquot'): 1,\n",
       " ('weight', 'seafood'): 1,\n",
       " ('make', 'daily'): 1,\n",
       " ('make', 'food'): 18,\n",
       " ('make', 'group'): 1,\n",
       " ('make', 'true'): 1,\n",
       " ('make', 'feel'): 3,\n",
       " ('make', 'little'): 2,\n",
       " ('make', 'bit'): 1,\n",
       " ('make', 'better'): 3,\n",
       " ('make', 'one'): 2,\n",
       " ('make', 'drive'): 1,\n",
       " ('make', 'thru'): 1,\n",
       " ('make', 'line'): 1,\n",
       " ('make', 'bring'): 1,\n",
       " ('make', 'house'): 2,\n",
       " ('make', 'foods'): 2,\n",
       " ('make', 'skip'): 1,\n",
       " ('make', 'episodes'): 1,\n",
       " ('make', 'play'): 1,\n",
       " ('make', 'open'): 1,\n",
       " ('make', 'im'): 1,\n",
       " ('make', 'watch'): 1,\n",
       " ('make', 'star'): 1,\n",
       " ('make', 'wars'): 1,\n",
       " ('make', 'get'): 2,\n",
       " ('make', 'mumbai'): 1,\n",
       " ('make', 'sooner'): 1,\n",
       " ('make', 'experience'): 1,\n",
       " ('make', 'summers'): 1,\n",
       " ('make', 'san'): 1,\n",
       " ('make', 'marcos'): 1,\n",
       " ('make', 'lovely'): 1,\n",
       " ('make', 'u'): 1,\n",
       " ('make', 'good'): 5,\n",
       " ('make', 'best'): 2,\n",
       " ('make', 'friend'): 1,\n",
       " ('make', 'cheddar'): 1,\n",
       " ('make', 'corn'): 1,\n",
       " ('make', 'potato'): 1,\n",
       " ('make', 'amp'): 1,\n",
       " ('make', 'lemon'): 1,\n",
       " ('make', 'potatoes'): 1,\n",
       " ('make', 'today'): 3,\n",
       " ('make', 'right'): 1,\n",
       " ('make', 'fast'): 1,\n",
       " ('make', 'lots'): 1,\n",
       " ('make', 'money'): 1,\n",
       " ('make', 'delicious'): 1,\n",
       " ('make', 'mexican'): 1,\n",
       " ('make', 'much'): 1,\n",
       " ('make', 'tonight'): 1,\n",
       " ('make', 'hope'): 1,\n",
       " ('make', 'guys'): 1,\n",
       " ('make', 'fun'): 1,\n",
       " ('make', 'soon'): 2,\n",
       " ('make', 'easier'): 1,\n",
       " ('make', 'spoil'): 1,\n",
       " ('make', 'girls'): 1,\n",
       " ('make', 'sure'): 2,\n",
       " ('make', 'foodwater'): 1,\n",
       " ('make', 'attention'): 1,\n",
       " ('make', 'smile'): 1,\n",
       " ('make', 'super'): 1,\n",
       " ('make', 'quorn'): 1,\n",
       " ('make', 'bolognese'): 1,\n",
       " ('make', 'lol'): 2,\n",
       " ('make', 'streaming'): 1,\n",
       " ('make', 'never'): 1,\n",
       " ('make', 'back'): 1,\n",
       " ('make', 'family'): 1,\n",
       " ('make', 'want'): 1,\n",
       " ('make', 'buy'): 1,\n",
       " ('make', 'thingsso'): 1,\n",
       " ('make', 'eat'): 1,\n",
       " ('make', 'more'): 1,\n",
       " ('make', 'informative'): 1,\n",
       " ('make', 'speech'): 1,\n",
       " ('make', 'gross'): 1,\n",
       " ('make', 'jookern'): 1,\n",
       " ('make', 'fucking'): 1,\n",
       " ('make', 'around'): 1,\n",
       " ('make', 'dinner'): 1,\n",
       " ('make', 'since'): 1,\n",
       " ('make', 'war'): 1,\n",
       " ('make', 'mum'): 1,\n",
       " ('make', 'pretty'): 1,\n",
       " ('make', 'like'): 2,\n",
       " ('make', 'mine'): 1,\n",
       " ('make', 'nail'): 1,\n",
       " ('make', 'salon'): 1,\n",
       " ('make', 'yet'): 1,\n",
       " ('make', 'need'): 1,\n",
       " ('make', 'heck'): 1,\n",
       " ('make', 'couple'): 1,\n",
       " ('make', 'superfood'): 1,\n",
       " ('make', 'new'): 1,\n",
       " ('make', 'something'): 3,\n",
       " ('make', 'parsnips'): 1,\n",
       " ('make', 'save'): 1,\n",
       " ('make', 'too'): 1,\n",
       " ('make', 'coconut'): 1,\n",
       " ('make', 'pancakes'): 1,\n",
       " ('make', 'really'): 2,\n",
       " ('make', 'sick'): 1,\n",
       " ('make', 'taste'): 1,\n",
       " ('make', 'anything'): 1,\n",
       " ('make', 'chef'): 1,\n",
       " ('make', 'me'): 1,\n",
       " ('make', 'bbq'): 1,\n",
       " ('make', 'peace'): 1,\n",
       " ('make', 'chicago'): 1,\n",
       " ('make', 'town'): 1,\n",
       " ('make', 'pizzas'): 1,\n",
       " ('make', 'take'): 1,\n",
       " ('make', 'hour'): 1,\n",
       " ('make', 'cake'): 1,\n",
       " ('make', 'network'): 1,\n",
       " ('make', 'fancy'): 1,\n",
       " ('make', 'time'): 1,\n",
       " ('make', 'cos'): 1,\n",
       " ('make', 'wont'): 1,\n",
       " ('make', 'share'): 1,\n",
       " ('make', 'happy'): 1,\n",
       " ('make', 'guy'): 1,\n",
       " ('make', 'gotta'): 1,\n",
       " ('make', 'go'): 1,\n",
       " ('make', 'urgh'): 1,\n",
       " ('make', 'think'): 1,\n",
       " ('make', 'it'): 1,\n",
       " ('make', 'match'): 1,\n",
       " ('daily', 'food'): 2,\n",
       " ('daily', 'group'): 1,\n",
       " ('daily', 'true'): 1,\n",
       " ('daily', 'story'): 1,\n",
       " ('daily', 'digital'): 1,\n",
       " ('daily', 'mec'): 1,\n",
       " ('daily', 'interaction'): 1,\n",
       " ('food', 'group'): 4,\n",
       " ('food', 'true'): 1,\n",
       " ('food', 'story'): 1,\n",
       " ('food', 'eat'): 26,\n",
       " ('food', 'oh'): 8,\n",
       " ('food', 'well'): 13,\n",
       " ('food', 'finsh'): 1,\n",
       " ('food', 'haha'): 17,\n",
       " ('food', 'pleeeaseee'): 1,\n",
       " ('food', 'love'): 27,\n",
       " ('food', 'pf'): 1,\n",
       " ('food', 'changs'): 1,\n",
       " ('food', 'always'): 9,\n",
       " ('food', 'perks'): 1,\n",
       " ('food', 'im'): 48,\n",
       " ('food', 'gettin'): 2,\n",
       " ('food', 'food'): 11,\n",
       " ('food', 'poisoning'): 7,\n",
       " ('food', 'hope'): 7,\n",
       " ('food', 'pr0n'): 1,\n",
       " ('food', 'museum'): 1,\n",
       " ('food', 'zoo'): 1,\n",
       " ('food', 'come'): 13,\n",
       " ('food', 'lol'): 44,\n",
       " ('food', 'hmm'): 1,\n",
       " ('food', 'may'): 4,\n",
       " ('food', 'b'): 3,\n",
       " ('food', 'yum'): 8,\n",
       " ('food', 'soo'): 3,\n",
       " ('food', 'stuffed'): 2,\n",
       " ('food', 'places'): 3,\n",
       " ('food', 'nyc'): 2,\n",
       " ('food', 'tummy'): 3,\n",
       " ('food', 'good'): 75,\n",
       " ('food', 'day'): 24,\n",
       " ('food', 'httptwitpiccom6qi0j'): 1,\n",
       " ('food', 'dinnerjapanese'): 1,\n",
       " ('food', 'were'): 3,\n",
       " ('food', 'beer'): 14,\n",
       " ('food', 'happy'): 16,\n",
       " ('food', 'bday'): 1,\n",
       " ('food', 'amp'): 51,\n",
       " ('food', 'movie'): 11,\n",
       " ('food', 'catch'): 1,\n",
       " ('food', 'pick'): 1,\n",
       " ('food', 'thingidea'): 1,\n",
       " ('food', 'ps'): 1,\n",
       " ('food', 'ive'): 6,\n",
       " ('food', 'school'): 4,\n",
       " ('food', 'starve'): 2,\n",
       " ('food', 'time'): 30,\n",
       " ('food', 'pizza'): 4,\n",
       " ('food', 'hhah'): 1,\n",
       " ('food', 'lucky'): 1,\n",
       " ('food', 'loving'): 3,\n",
       " ('food', 'scoffing'): 1,\n",
       " ('food', 'brownies'): 2,\n",
       " ('food', 'enjoying'): 2,\n",
       " ('food', 'lebanese'): 1,\n",
       " ('food', 'raiyu'): 1,\n",
       " ('food', 'congrats'): 1,\n",
       " ('food', 'yums'): 1,\n",
       " ('food', 'anxiously'): 1,\n",
       " ('food', 'awaiting'): 1,\n",
       " ('food', 'berries'): 1,\n",
       " ('food', 'enjoy'): 8,\n",
       " ('food', 'stay'): 1,\n",
       " ('food', 'here'): 10,\n",
       " ('food', 'away'): 1,\n",
       " ('food', 'tomorrow'): 9,\n",
       " ('food', 'maybe'): 11,\n",
       " ('food', 'leave'): 3,\n",
       " ('food', 'it'): 7,\n",
       " ('food', 'yet'): 5,\n",
       " ('food', 'maccy'): 1,\n",
       " ('food', 'ds'): 1,\n",
       " ('food', 'sainsburys'): 1,\n",
       " ('food', 'mmm'): 7,\n",
       " ('food', 'barbeque'): 1,\n",
       " ('food', 'finally'): 2,\n",
       " ('food', 'bad'): 10,\n",
       " ('food', 'carlos'): 1,\n",
       " ('food', 'cant'): 10,\n",
       " ('food', 'emte'): 1,\n",
       " ('food', 'get'): 22,\n",
       " ('food', '10'): 4,\n",
       " ('food', 'every'): 3,\n",
       " ('food', 'e'): 1,\n",
       " ('food', 'past'): 4,\n",
       " ('food', 'days'): 3,\n",
       " ('food', 'guessedu'): 1,\n",
       " ('food', 'know'): 11,\n",
       " ('food', 'ann'): 1,\n",
       " ('food', 'lyss'): 1,\n",
       " ('food', 'work'): 12,\n",
       " ('food', 'brett'): 1,\n",
       " ('food', 'right'): 12,\n",
       " ('food', 'winter'): 1,\n",
       " ('food', 'already'): 7,\n",
       " ('food', 'summer'): 2,\n",
       " ('food', 'yom'): 2,\n",
       " ('food', 'httptinyurlcomml7ut7'): 1,\n",
       " ('food', 'ian'): 1,\n",
       " ('food', 'see'): 8,\n",
       " ('food', 'mmmmm'): 3,\n",
       " ('food', 'smells'): 3,\n",
       " ('food', 'home'): 10,\n",
       " ('food', 'cooked'): 2,\n",
       " ('food', 'meal'): 2,\n",
       " ('food', 'want'): 13,\n",
       " ('food', 'sleep'): 7,\n",
       " ('food', 'sickness'): 1,\n",
       " ('food', 'long'): 3,\n",
       " ('food', 'train'): 1,\n",
       " ('food', 'stranraer'): 1,\n",
       " ('food', 'city'): 1,\n",
       " ('food', 'island'): 1,\n",
       " ('food', 'today'): 28,\n",
       " ('food', 'would'): 3,\n",
       " ('food', '2gether'): 1,\n",
       " ('food', 'kfcphilippines'): 1,\n",
       " ('food', 'ano'): 1,\n",
       " ('food', 'mas'): 1,\n",
       " ('food', 'masarap'): 1,\n",
       " ('food', 'turns'): 1,\n",
       " ('food', 'u'): 10,\n",
       " ('food', 'naww'): 1,\n",
       " ('food', 'dnt'): 1,\n",
       " ('food', 'paradises'): 1,\n",
       " ('food', 'sandwiches'): 1,\n",
       " ('food', 'eps'): 1,\n",
       " ('food', 'gahhhh'): 1,\n",
       " ('food', 'flown'): 1,\n",
       " ('food', 'caribbean'): 1,\n",
       " ('food', 'cheaper'): 2,\n",
       " ('food', 'potato'): 1,\n",
       " ('food', 'going'): 14,\n",
       " ('food', 'take'): 6,\n",
       " ('food', 'last'): 7,\n",
       " ('food', 'final'): 1,\n",
       " ('food', 'olive'): 1,\n",
       " ('food', 'garden'): 4,\n",
       " ('food', 'jeremy'): 1,\n",
       " ('food', 'kyle'): 1,\n",
       " ('food', 'shopping'): 17,\n",
       " ('food', 'abt'): 1,\n",
       " ('food', '3'): 3,\n",
       " ('food', 'months'): 2,\n",
       " ('food', 'far'): 2,\n",
       " ('food', 'east'): 1,\n",
       " ('food', 'plaza'): 1,\n",
       " ('food', 'woo'): 1,\n",
       " ('food', 'needy'): 1,\n",
       " ('food', 'dc'): 1,\n",
       " ('food', 'network'): 18,\n",
       " ('food', 'star'): 5,\n",
       " ('food', 'denise'): 1,\n",
       " ('food', 'richards'): 1,\n",
       " ('food', 'trust'): 1,\n",
       " ('food', 'cooking'): 4,\n",
       " ('food', 'exercise'): 2,\n",
       " ('food', 'gus'): 1,\n",
       " ('food', 'greek'): 1,\n",
       " ('food', 'emilys'): 1,\n",
       " ('food', 'birthday'): 4,\n",
       " ('food', 'party'): 7,\n",
       " ('food', 'mmmmmbrownies'): 1,\n",
       " ('food', 'paradise'): 1,\n",
       " ('food', 'though'): 14,\n",
       " ('food', 'feel'): 5,\n",
       " ('food', 'guilty'): 1,\n",
       " ('food', 'court'): 11,\n",
       " ('food', 'listening'): 2,\n",
       " ('food', 'high'): 3,\n",
       " ('food', 'schoolish'): 1,\n",
       " ('food', 'friendship'): 2,\n",
       " ('food', 'little'): 6,\n",
       " ('food', 'teaching'): 1,\n",
       " ('food', 'end'): 3,\n",
       " ('food', 'makes'): 14,\n",
       " ('food', 'better'): 14,\n",
       " ('food', 'yes'): 6,\n",
       " ('food', 'cook'): 3,\n",
       " ('food', 'properly'): 1,\n",
       " ('food', 'etc'): 2,\n",
       " ('food', 'okaythanksbye'): 1,\n",
       " ('food', 'delicious'): 7,\n",
       " ('food', 'brb'): 4,\n",
       " ('food', 'spicy'): 2,\n",
       " ('food', 'fried'): 3,\n",
       " ('food', 'tofu'): 1,\n",
       " ('food', 'bring'): 7,\n",
       " ('food', 'house'): 8,\n",
       " ('food', 'super'): 4,\n",
       " ('food', 'great'): 38,\n",
       " ('food', 'hit'): 3,\n",
       " ('food', 'look'): 7,\n",
       " ('food', 'like'): 15,\n",
       " ('food', 'advertisment'): 1,\n",
       " ('food', 'monsoon'): 1,\n",
       " ('food', 'attack'): 1,\n",
       " ('food', 'taste'): 5,\n",
       " ('food', 'eatten'): 1,\n",
       " ('food', 'quack'): 1,\n",
       " ('food', 'list'): 3,\n",
       " ('food', '1mcdonalds'): 1,\n",
       " ('food', '2deep'): 1,\n",
       " ('food', 'word'): 1,\n",
       " ('food', 'nap'): 3,\n",
       " ('food', 'later'): 12,\n",
       " ('food', 'big'): 4,\n",
       " ('food', 'foot'): 1,\n",
       " ('food', 'mama'): 1,\n",
       " ('food', 'drank'): 1,\n",
       " ('food', 'nice'): 16,\n",
       " ('food', 'wine'): 14,\n",
       " ('food', 'things'): 2,\n",
       " ('food', 'service'): 5,\n",
       " ('food', 'margaritas'): 1,\n",
       " ('food', 'starving'): 6,\n",
       " ('food', 'times'): 9,\n",
       " ('food', 'top'): 1,\n",
       " ('food', 'off'): 1,\n",
       " ('food', 'got'): 13,\n",
       " ('food', 'still'): 13,\n",
       " ('food', 'think'): 8,\n",
       " ('food', 'need'): 10,\n",
       " ('food', 'yummy'): 17,\n",
       " ('food', 'korea'): 1,\n",
       " ('food', 'eating'): 10,\n",
       " ('food', 'there'): 6,\n",
       " ('food', 'leaving'): 2,\n",
       " ('food', 'go'): 16,\n",
       " ('food', 'hospital'): 1,\n",
       " ('food', 'cart'): 1,\n",
       " ('food', 'suggestions'): 2,\n",
       " ('food', 'ill'): 17,\n",
       " ('food', 'thanks'): 8,\n",
       " ('food', 'recomendation'): 1,\n",
       " ('food', 'ick'): 1,\n",
       " ('food', 'reviewthe'): 1,\n",
       " ('food', 'pains'): 1,\n",
       " ('food', 'job'): 1,\n",
       " ('food', 'tum'): 1,\n",
       " ('food', 'spam'): 1,\n",
       " ('food', 'knew'): 1,\n",
       " ('food', 'follow'): 3,\n",
       " ('food', 'you'): 7,\n",
       " ('food', 'sri'): 1,\n",
       " ('food', 'lanka'): 1,\n",
       " ('food', 'india'): 1,\n",
       " ('food', 'including'): 1,\n",
       " ('food', 'sunny'): 2,\n",
       " ('food', 'getting'): 3,\n",
       " ('food', 'new'): 5,\n",
       " ('food', 'restaurant'): 2,\n",
       " ('food', 'yay'): 9,\n",
       " ('food', 'makeuphair'): 1,\n",
       " ('food', 'trial'): 2,\n",
       " ('food', 'w'): 8,\n",
       " ('food', 'bff'): 1,\n",
       " ('food', 'volunteers'): 1,\n",
       " ('food', 'stayed'): 2,\n",
       " ('food', 'meat'): 2,\n",
       " ('food', 'homemade'): 3,\n",
       " ('food', 'fries'): 2,\n",
       " ('food', 'open'): 2,\n",
       " ('food', 'cold'): 2,\n",
       " ('food', 'huge'): 1,\n",
       " ('food', 'thumbs'): 1,\n",
       " ('food', 'coconut'): 1,\n",
       " ('food', 'fun'): 19,\n",
       " ('food', 'rule'): 1,\n",
       " ('food', 'middle'): 1,\n",
       " ('food', 'wales'): 1,\n",
       " ('food', 'typical'): 1,\n",
       " ('food', 'huh'): 1,\n",
       " ('food', 'full'): 4,\n",
       " ('food', 'birds'): 2,\n",
       " ('food', 'watch'): 4,\n",
       " ('food', 'wars'): 1,\n",
       " ('food', 'empire'): 1,\n",
       " ('food', '7'): 1,\n",
       " ('food', 'am'): 1,\n",
       " ('food', 'drinks'): 10,\n",
       " ('food', 'celebrate'): 2,\n",
       " ('food', 'sisters'): 4,\n",
       " ('food', 'graduation'): 1,\n",
       " ('food', 'hahaha'): 4,\n",
       " ('food', 'alladins'): 1,\n",
       " ('food', 'shame'): 2,\n",
       " ('food', 'ready'): 8,\n",
       " ('food', 'song'): 2,\n",
       " ('food', 'festivalswell'): 1,\n",
       " ('food', 'fests'): 1,\n",
       " ('food', 'and'): 3,\n",
       " ('food', 'learn'): 2,\n",
       " ('food', 'leadership'): 1,\n",
       " ('food', 'course'): 7,\n",
       " ('food', 'finale'): 1,\n",
       " ('food', 'gossip'): 1,\n",
       " ('food', 'girl'): 6,\n",
       " ('food', 'helps'): 2,\n",
       " ('food', 'short'): 1,\n",
       " ('food', 'term'): 1,\n",
       " ('food', 'writing'): 1,\n",
       " ('food', 'box'): 1,\n",
       " ('food', 'farm'): 1,\n",
       " ('food', 'share'): 1,\n",
       " ('food', 'morning'): 2,\n",
       " ('food', 'run'): 3,\n",
       " ('food', 'preparation'): 1,\n",
       " ('food', 'bsg'): 1,\n",
       " ('food', 'venue'): 2,\n",
       " ('food', 'theyve'): 1,\n",
       " ('food', 'bbq'): 7,\n",
       " ('food', 'start'): 5,\n",
       " ('food', 'goodbye'): 1,\n",
       " ('food', 'diet'): 1,\n",
       " ('food', 'stamps'): 2,\n",
       " ('food', 'prove'): 1,\n",
       " ('food', 'exist'): 1,\n",
       " ('food', 'first'): 7,\n",
       " ('food', 'blogs'): 2,\n",
       " ('food', 'could'): 6,\n",
       " ('food', 'find'): 2,\n",
       " ('food', 'one'): 6,\n",
       " ('food', 'join'): 1,\n",
       " ('food', 'us'): 4,\n",
       " ('food', 'table'): 2,\n",
       " ('food', 'anything'): 5,\n",
       " ('food', 'sorryquot'): 1,\n",
       " ('food', 'homecooked'): 1,\n",
       " ('food', 'games'): 3,\n",
       " ('food', 'years'): 2,\n",
       " ('food', 'hmmm'): 2,\n",
       " ('food', 'cali'): 1,\n",
       " ('food', 'next'): 9,\n",
       " ('food', 'looks'): 4,\n",
       " ('food', 'really'): 10,\n",
       " ('food', 'lack'): 1,\n",
       " ('food', 'of'): 1,\n",
       " ('food', 'mean'): 2,\n",
       " ('food', 'quotyou'): 1,\n",
       " ('food', 'virtual'): 1,\n",
       " ('food', 'tech'): 6,\n",
       " ('food', 'atm'): 2,\n",
       " ('food', 'weeeheeyy'): 1,\n",
       " ('food', 'lovely'): 2,\n",
       " ('food', 'friends'): 15,\n",
       " ('food', 'make'): 19,\n",
       " ('food', 'summers'): 1,\n",
       " ('food', 'seems'): 1,\n",
       " ('food', 'important'): 1,\n",
       " ('food', 'me'): 7,\n",
       " ('food', 'keema'): 1,\n",
       " ('food', 'world'): 4,\n",
       " ('food', 'waiting'): 4,\n",
       " ('food', 'night'): 9,\n",
       " ('food', 'needs'): 2,\n",
       " ('food', 'bed'): 8,\n",
       " ('food', 'thirsty'): 1,\n",
       " ('food', 'took'): 2,\n",
       " ('food', 'whilst'): 2,\n",
       " ('food', 'jeep'): 1,\n",
       " ('food', 'complain'): 2,\n",
       " ('food', 'fighting'): 1,\n",
       " ('food', 'words'): 2,\n",
       " ('food', 'mood'): 1,\n",
       " ('food', 'nevermind'): 1,\n",
       " ('food', 'b4'): 1,\n",
       " ('food', 'workguess'): 1,\n",
       " ('food', 'aint'): 1,\n",
       " ('food', 'ahhhh'): 1,\n",
       " ('food', 'adam'): 1,\n",
       " ('food', 'porn'): 6,\n",
       " ('food', 'session'): 1,\n",
       " ('food', 'conference'): 1,\n",
       " ('food', 'hungry'): 12,\n",
       " ('food', 'hot'): 3,\n",
       " ('food', 'now'): 13,\n",
       " ('food', 'httptweetsg'): 3,\n",
       " ('food', 'importantly'): 2,\n",
       " ('food', '4yr'): 1,\n",
       " ('food', 'followed'): 1,\n",
       " ('food', 'laugh'): 2,\n",
       " ('food', 'bowling'): 3,\n",
       " ('food', 'sunshine'): 3,\n",
       " ('food', 'pink'): 1,\n",
       " ('food', 'people'): 10,\n",
       " ('food', 'even'): 5,\n",
       " ('food', '13'): 1,\n",
       " ('food', 'pics'): 1,\n",
       " ('food', 'funny'): 1,\n",
       " ('food', 'creative'): 1,\n",
       " ('food', 'coma'): 9,\n",
       " ('food', 'bedtime'): 3,\n",
       " ('food', 'much'): 18,\n",
       " ('food', 'lunch'): 10,\n",
       " ('food', 'walked'): 1,\n",
       " ('food', 'beach'): 4,\n",
       " ('food', 'incquot'): 1,\n",
       " ('food', 'interesting'): 1,\n",
       " ('food', 'httptinyurlcomd9cmmv'): 1,\n",
       " ('food', 'vbs'): 1,\n",
       " ('food', 'decor'): 1,\n",
       " ('food', 'heading'): 2,\n",
       " ('food', 'baby'): 3,\n",
       " ('food', 'company'): 9,\n",
       " ('food', 'roommate'): 2,\n",
       " ('food', 'smell'): 1,\n",
       " ('food', 'tasty'): 1,\n",
       " ('food', 'best'): 9,\n",
       " ('food', 'friend'): 6,\n",
       " ('food', 'therapy'): 1,\n",
       " ('food', 'tara'): 2,\n",
       " ('food', 'real'): 3,\n",
       " ('food', 'fat'): 4,\n",
       " ('food', 'ass'): 1,\n",
       " ('food', 'its'): 1,\n",
       " ('food', 'dirty'): 1,\n",
       " ('food', 'convo'): 1,\n",
       " ('food', 'xdxxx'): 1,\n",
       " ('food', 'feelings'): 1,\n",
       " ('food', 'vs'): 1,\n",
       " ('food', 'russell'): 1,\n",
       " ('food', 'brand'): 1,\n",
       " ('food', 'tough'): 1,\n",
       " ('food', 'runs'): 1,\n",
       " ('food', 'music'): 4,\n",
       " ('food', 'banks'): 1,\n",
       " ('food', 'wordshop'): 1,\n",
       " ('food', 'wed'): 1,\n",
       " ('food', 'dad'): 2,\n",
       " ('food', 'origami'): 1,\n",
       " ('food', 'celebration'): 2,\n",
       " ('food', 'vegetarians'): 1,\n",
       " ('food', 'thank'): 3,\n",
       " ('food', 'god'): 3,\n",
       " ('food', 'book'): 4,\n",
       " ('food', 'easy'): 1,\n",
       " ('food', 'south'): 1,\n",
       " ('food', 'jersey'): 1,\n",
       " ('food', 'thai'): 2,\n",
       " ('food', 'cafe'): 2,\n",
       " ('food', 'westwood'): 1,\n",
       " ('food', 'blvd'): 1,\n",
       " ('food', 'everyone'): 4,\n",
       " ('food', 'else'): 5,\n",
       " ('food', 'please'): 6,\n",
       " ('food', 'dinner'): 9,\n",
       " ('food', 'ftw'): 2,\n",
       " ('food', 'help'): 1,\n",
       " ('food', 'hangover'): 1,\n",
       " ('food', 'art'): 2,\n",
       " ('food', 'snack'): 1,\n",
       " ('food', 'tonight'): 15,\n",
       " ('food', 'pop'): 1,\n",
       " ('food', 'fight'): 5,\n",
       " ('food', 'then'): 2,\n",
       " ('food', 'kinda'): 1,\n",
       " ('food', 'sg'): 1,\n",
       " ('food', 'daddy'): 2,\n",
       " ('food', 'bc'): 1,\n",
       " ('food', 'ct'): 1,\n",
       " ('food', 'goin'): 1,\n",
       " ('food', 'avb'): 1,\n",
       " ('food', 'amazing'): 11,\n",
       " ('food', 'least'): 5,\n",
       " ('food', 'tastes'): 2,\n",
       " ('food', 'ever'): 6,\n",
       " ('food', 'celes'): 1,\n",
       " ('food', 'xoxo'): 3,\n",
       " ('food', 'cheddar'): 1,\n",
       " ('food', 'corn'): 1,\n",
       " ('food', 'banking'): 1,\n",
       " ('food', 'sarah'): 2,\n",
       " ('food', 'sand'): 1,\n",
       " ('food', 'beasts'): 1,\n",
       " ('food', 'sex'): 1,\n",
       " ('food', 'security'): 1,\n",
       " ('food', 'motivates'): 1,\n",
       " ('food', 'digital'): 1,\n",
       " ('food', 'mec'): 1,\n",
       " ('food', 'interaction'): 1,\n",
       " ('food', 'wooo'): 1,\n",
       " ('food', 'phone'): 2,\n",
       " ('food', 'conversations'): 1,\n",
       " ('food', 'tv'): 6,\n",
       " ('food', 'shape'): 1,\n",
       " ('food', 'fck'): 1,\n",
       " ('food', 'buti'): 1,\n",
       " ('food', 'sooooo'): 1,\n",
       " ('food', 'ate'): 2,\n",
       " ('food', 'oscarmayer'): 1,\n",
       " ('food', 'goodnight'): 2,\n",
       " ('food', 'made'): 4,\n",
       " ('food', 'white'): 2,\n",
       " ('food', 'girliesgood'): 1,\n",
       " ('food', 'excuse'): 1,\n",
       " ('food', 'soul'): 3,\n",
       " ('food', 'idea'): 2,\n",
       " ('food', 'something'): 4,\n",
       " ('food', 'yesterday'): 3,\n",
       " ('food', 'dear'): 1,\n",
       " ('food', 'hair'): 2,\n",
       " ('food', 'fir'): 1,\n",
       " ('food', 'grad'): 2,\n",
       " ('food', 'dropped'): 1,\n",
       " ('food', 'batting'): 1,\n",
       " ('food', 'around'): 10,\n",
       " ('food', 'floor'): 1,\n",
       " ('food', 'back'): 13,\n",
       " ('food', 'woot'): 2,\n",
       " ('food', 'lots'): 8,\n",
       " ('food', 'mowed'): 1,\n",
       " ('food', 'attractive'): 1,\n",
       " ('food', 'bleu'): 1,\n",
       " ('food', 'columbia'): 1,\n",
       " ('food', 'httpyfrogcom5ewohj'): 1,\n",
       " ('food', 'gave'): 2,\n",
       " ('food', 'awards'): 1,\n",
       " ('food', 'honored'): 1,\n",
       " ('food', 'finalists'): 1,\n",
       " ('food', 'crepes'): 1,\n",
       " ('food', 'likely'): 1,\n",
       " ('food', 'isnt'): 1,\n",
       " ('food', 'string'): 1,\n",
       " ('food', 'drink'): 11,\n",
       " ('food', 'underworld'): 1,\n",
       " ('food', 'definite'): 1,\n",
       " ('food', 'no'): 1,\n",
       " ('food', 'gatorade'): 1,\n",
       " ('food', 'salvation'): 1,\n",
       " ('food', 'rated'): 1,\n",
       " ('food', 'order'): 4,\n",
       " ('food', 'ipod'): 3,\n",
       " ('food', 'literally'): 2,\n",
       " ('food', 'finished'): 1,\n",
       " ('food', 'alcohol'): 2,\n",
       " ('food', 'heaven'): 2,\n",
       " ('food', 'britneh'): 1,\n",
       " ('food', 'wedding'): 2,\n",
       " ('food', 'camping'): 1,\n",
       " ('food', 'choices'): 2,\n",
       " ('food', 'race'): 2,\n",
       " ('food', 'weekend'): 4,\n",
       " ('food', 'awesome'): 13,\n",
       " ('food', 'carne'): 1,\n",
       " ('food', 'bk'): 1,\n",
       " ('food', 'life'): 5,\n",
       " ('food', 'simpler'): 1,\n",
       " ('food', 'nut'): 1,\n",
       " ('food', 'thats'): 8,\n",
       " ('food', 'fruit'): 2,\n",
       " ('food', 'napa'): 1,\n",
       " ('food', 'jimming'): 1,\n",
       " ('food', 'establishment'): 1,\n",
       " ('food', 'bloating'): 1,\n",
       " ('food', 'andor'): 2,\n",
       " ('food', 'indigestion'): 1,\n",
       " ('food', 'playing'): 1,\n",
       " ('food', 'troops'): 1,\n",
       " ('food', 'aghast'): 1,\n",
       " ('food', 'dutch'): 1,\n",
       " ('food', 'httptwurlnlgbns1g'): 1,\n",
       " ('food', 'httpyfrogcom16483mj'): 1,\n",
       " ('food', 'sounds'): 7,\n",
       " ('food', 'meee'): 1,\n",
       " ('food', 'action'): 1,\n",
       " ('food', 'watching'): 4,\n",
       " ('food', 'sense'): 1,\n",
       " ('food', 'indian'): 2,\n",
       " ('food', 'butter'): 1,\n",
       " ('food', 'naan'): 1,\n",
       " ('food', 'boardgames'): 1,\n",
       " ('food', 'soon'): 6,\n",
       " ('food', 'cheer'): 2,\n",
       " ('food', 'hb4'): 1,\n",
       " ('food', 'gt'): 2,\n",
       " ('food', 'cendol'): 1,\n",
       " ('food', 'cuddling'): 1,\n",
       " ('food', 'babies'): 1,\n",
       " ('food', 'nearby'): 1,\n",
       " ('food', 'office'): 1,\n",
       " ('food', 'coffee'): 3,\n",
       " ('food', 'money'): 3,\n",
       " ('food', 'greetings'): 1,\n",
       " ('food', 'doubt'): 1,\n",
       " ('food', 'tried'): 3,\n",
       " ('food', 'cigarettes'): 1,\n",
       " ('food', 'miles'): 1,\n",
       " ('food', 'odometer'): 1,\n",
       " ('food', 'batteries'): 1,\n",
       " ('food', 'dyes'): 1,\n",
       " ('food', 'ingredientsy'): 1,\n",
       " ('food', 'beetroots'): 1,\n",
       " ('food', 'bitd'): 1,\n",
       " ('food', 'stomach'): 2,\n",
       " ('food', 'girlie'): 1,\n",
       " ('food', 'renegade'): 1,\n",
       " ('food', 'carla'): 1,\n",
       " ('food', 'golden'): 1,\n",
       " ('food', 'feels'): 1,\n",
       " ('food', 'light'): 1,\n",
       " ('food', 'surely'): 1,\n",
       " ('food', 'nation'): 2,\n",
       " ('food', 'changed'): 1,\n",
       " ('food', 'dundee'): 1,\n",
       " ('food', 'glorious'): 2,\n",
       " ('food', 'definition'): 1,\n",
       " ('food', 'nick'): 2,\n",
       " ('food', 'fails'): 1,\n",
       " ('food', 'touch'): 1,\n",
       " ('food', 'ooooohhhh'): 1,\n",
       " ('food', 'httptwitpiccom6j59j'): 1,\n",
       " ('food', 'makeup'): 1,\n",
       " ('food', 'clothes'): 3,\n",
       " ('food', 'lt3'): 7,\n",
       " ('food', 'sort'): 1,\n",
       " ('food', 'managed'): 1,\n",
       " ('food', 'hahaa'): 1,\n",
       " ('food', 'snappin'): 1,\n",
       " ('food', 'sophs'): 1,\n",
       " ('food', 'bampw'): 2,\n",
       " ('food', 'worry'): 2,\n",
       " ('food', 'iphone'): 1,\n",
       " ('food', 'bye'): 2,\n",
       " ('food', 'counting'): 1,\n",
       " ('food', 'means'): 1,\n",
       " ('food', 'offer'): 1,\n",
       " ('food', 'wwwgrazecom'): 1,\n",
       " ('food', 'code'): 1,\n",
       " ('food', 'pgt5mb8t'): 1,\n",
       " ('food', 'comas'): 2,\n",
       " ('food', 'given'): 1,\n",
       " ('food', 'unconscious'): 1,\n",
       " ('food', 'bloggers'): 1,\n",
       " ('food', 'favorite'): 3,\n",
       " ('food', 'peeps'): 1,\n",
       " ('food', 'angle'): 1,\n",
       " ('food', 'never'): 9,\n",
       " ('food', 'bored'): 2,\n",
       " ('food', 'mmmm'): 2,\n",
       " ('food', 'chinese'): 4,\n",
       " ('food', 'equally'): 1,\n",
       " ('food', 'fridge'): 5,\n",
       " ('food', 'allowed'): 1,\n",
       " ('food', 'morton'): 1,\n",
       " ('food', 'wont'): 1,\n",
       " ('food', 'yumm'): 2,\n",
       " ('food', 'ridicuousss'): 1,\n",
       " ('food', 'pictures'): 3,\n",
       " ('food', 'gonna'): 6,\n",
       " ('food', 'hittin'): 1,\n",
       " ('food', 'spots'): 2,\n",
       " ('food', 'eeeasy'): 1,\n",
       " ('food', 'town'): 3,\n",
       " ('food', 'prepare'): 1,\n",
       " ('food', 'twickpix'): 1,\n",
       " ('food', '10pm'): 1,\n",
       " ('food', 'anyway'): 3,\n",
       " ('food', 'marks'): 1,\n",
       " ('food', 'spencers'): 1,\n",
       " ('food', 'freezing'): 1,\n",
       " ('food', 'dont'): 3,\n",
       " ('food', 'mess'): 1,\n",
       " ('food', 'either'): 2,\n",
       " ('food', 'noticed'): 1,\n",
       " ('food', 'shark'): 1,\n",
       " ('food', 'guru'): 1,\n",
       " ('food', 'httpisgdrauq'): 1,\n",
       " ('food', 'working'): 2,\n",
       " ('food', 'jean'): 1,\n",
       " ('food', 'brought'): 1,\n",
       " ('food', 'half'): 1,\n",
       " ('food', 'watermelon'): 1,\n",
       " ('food', 'stuff'): 4,\n",
       " ('food', 'way'): 6,\n",
       " ('food', 'option'): 2,\n",
       " ('food', 'necessarily'): 1,\n",
       " ('food', 'cause'): 5,\n",
       " ('food', 'ur'): 1,\n",
       " ('food', 'death'): 1,\n",
       " ('food', 'guyana'): 1,\n",
       " ('food', 'bomb'): 5,\n",
       " ('food', 'hung'): 1,\n",
       " ('food', 'nope'): 1,\n",
       " ('food', 'actually'): 2,\n",
       " ('food', 'bathroom'): 1,\n",
       " ('food', 'teehee'): 1,\n",
       " ('food', 'wassss'): 1,\n",
       " ('food', 'tan'): 2,\n",
       " ('food', 'havent'): 1,\n",
       " ('food', 'evon'): 1,\n",
       " ('food', 'done'): 4,\n",
       " ('food', 'page'): 1,\n",
       " ('food', 'bank'): 1,\n",
       " ('food', 'la'): 1,\n",
       " ('food', 'gotta'): 3,\n",
       " ('food', 'opera'): 1,\n",
       " ('food', 'joke'): 2,\n",
       " ('food', 'toooo'): 1,\n",
       " ('food', 'whatdya'): 1,\n",
       " ('food', 'water'): 2,\n",
       " ('food', 'looking'): 4,\n",
       " ('food', 'partner'): 1,\n",
       " ('food', 'teh'): 2,\n",
       " ('food', 'kotak'): 1,\n",
       " ('food', '2'): 6,\n",
       " ('food', 'guessing'): 1,\n",
       " ('food', 'epic'): 1,\n",
       " ('food', 'jk'): 1,\n",
       " ('food', 'miss'): 2,\n",
       " ('food', 'advice'): 1,\n",
       " ('food', 'shit'): 3,\n",
       " ('food', 'parentss'): 1,\n",
       " ('food', 'looked'): 1,\n",
       " ('food', 'entries'): 1,\n",
       " ('food', 'packing'): 1,\n",
       " ('food', 'dh'): 1,\n",
       " ('food', 'tween'): 1,\n",
       " ('food', 'moved'): 1,\n",
       " ('food', 'thought'): 7,\n",
       " ('food', 'feta'): 2,\n",
       " ('food', 'choriatiki'): 2,\n",
       " ('food', 'salata'): 2,\n",
       " ('food', 'brazil'): 1,\n",
       " ('food', 'different'): 3,\n",
       " ('food', 'mix'): 2,\n",
       " ('food', 'japonese'): 1,\n",
       " ('food', 'heavy'): 1,\n",
       " ...}"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.get_edge_attributes(posWordGraph, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.5864928909952607\n",
      "SVM score: 0.5746445497630331\n",
      "Logistic regression score: 0.591824644549763\n",
      "Decision tree score: 0.5746445497630331\n"
     ]
    }
   ],
   "source": [
    "posWordGraphTrimmed = removeEdgesByWeight(posWordGraph, 8)\n",
    "negWordGraphTrimmed = removeEdgesByWeight(negWordGraph, 8)\n",
    "\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraphTrimmed, negWordGraphTrimmed, \"edge\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraphTrimmed, negWordGraphTrimmed, \"edge\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "5272\n"
     ]
    }
   ],
   "source": [
    "print(len(negWordGraphTrimmed))\n",
    "print(len(negWordGraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.5438388625592417\n",
      "SVM score: 0.580568720379147\n",
      "Logistic regression score: 0.5817535545023697\n",
      "Decision tree score: 0.5545023696682464\n"
     ]
    }
   ],
   "source": [
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraphTrimmed, negWordGraphTrimmed, \"MCSNS\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraphTrimmed, negWordGraphTrimmed, \"MCSNS\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.5989336492890995\n",
      "SVM score: 0.6149289099526066\n",
      "Logistic regression score: 0.6137440758293838\n",
      "Decision tree score: 0.5598341232227488\n"
     ]
    }
   ],
   "source": [
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraphTrimmed, negWordGraphTrimmed, \"MCSUES\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraphTrimmed, negWordGraphTrimmed, \"MCSUES\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.5361374407582938\n",
      "SVM score: 0.5669431279620853\n",
      "Logistic regression score: 0.5728672985781991\n",
      "Decision tree score: 0.5207345971563981\n"
     ]
    }
   ],
   "source": [
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraphTrimmed, negWordGraphTrimmed, \"MCSDES\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraphTrimmed, negWordGraphTrimmed, \"MCSDES\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
