{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\miche\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import networkx as nx\n",
    "import re\n",
    "import math\n",
    "import glob\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "from sklearn.linear_model import LogisticRegression as logreg\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier as dectree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Homework Review Data\n",
    "Assigned a value of 4 to positive reviews and 0 to negative reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This homework annoying like every homeowork s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>This homework far hardest homework semester I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>HW1 relatively simple assignment complete lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>The first question felt overly tedious It dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>This homework difficult others Some questions...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0   This homework annoying like every homeowork s...\n",
       "1          0   This homework far hardest homework semester I...\n",
       "2          0   HW1 relatively simple assignment complete lot...\n",
       "3          0   The first question felt overly tedious It dif...\n",
       "4          0   This homework difficult others Some questions..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def removeStopWords(text):\n",
    "    #text = text.decode('UTF-8')\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    word_tokens = text.split() #split by white space\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = \"\"\n",
    "    i = 0\n",
    "    while i < len(word_tokens):\n",
    "        if word_tokens[i] not in stop_words:\n",
    "            filtered_sentence = filtered_sentence + \" \" + word_tokens[i]\n",
    "        i = i + 1\n",
    "    return filtered_sentence\n",
    "\n",
    "#Processes all negative reviews into on dataframe\n",
    "#pos = 4, neg = 2, neutral = 0\n",
    "path_FL16_neg= './dataset/hw_reviews/all_negative_reviews/fl16/*.txt'\n",
    "path_FL17_neg = './dataset/hw_reviews/all_negative_reviews/fl17/*.txt'\n",
    "path_FL18_neg = './dataset/hw_reviews/all_negative_reviews/fl18/*.txt'\n",
    "path_SP17_neg = './dataset/hw_reviews/all_negative_reviews/sp17/*.txt'\n",
    "path_SP18_neg = './dataset/hw_reviews/all_negative_reviews/sp18/*.txt'\n",
    "files = glob.glob(path_FL16_neg)\n",
    "files2 = glob.glob(path_FL17_neg)\n",
    "files3 = glob.glob(path_FL18_neg)\n",
    "files4 = glob.glob(path_SP17_neg)\n",
    "files5 = glob.glob(path_SP18_neg)\n",
    "negativeReviews = []\n",
    "for file in files:\n",
    "    f=open(file, encoding = 'utf-8')  \n",
    "    try:\n",
    "        text = f.read()\n",
    "    except:\n",
    "        text.encode('utf-8').strip()\n",
    "    filtered_Review = removeStopWords(text)\n",
    "    if len(filtered_Review) != 0:\n",
    "        negativeReviews.append(filtered_Review)\n",
    "    f.close() \n",
    "for file in files2:\n",
    "    f=open(file, encoding = 'utf-8')  \n",
    "    try:\n",
    "        text = f.read()\n",
    "    except:\n",
    "        text.encode('utf-8').strip()\n",
    "    filtered_Review = removeStopWords(text)\n",
    "    if len(filtered_Review) != 0:\n",
    "        negativeReviews.append(filtered_Review)\n",
    "    f.close() \n",
    "for file in files3:\n",
    "    f=open(file, encoding = 'utf-8')  \n",
    "    try:\n",
    "        text = f.read()\n",
    "    except:\n",
    "        text.encode('utf-8').strip()\n",
    "    filtered_Review = removeStopWords(text)\n",
    "    if len(filtered_Review) != 0:\n",
    "        negativeReviews.append(filtered_Review)\n",
    "    f.close() \n",
    "for file in files4:\n",
    "    f=open(file, encoding = 'utf-8')\n",
    "    try:\n",
    "        text = f.read()\n",
    "    except:\n",
    "        text.encode('utf-8').strip()\n",
    "    #text=str(text,'utf-8')\n",
    "    #text.encode('utf-8').strip()\n",
    "    filtered_Review = removeStopWords(text)\n",
    "    if len(filtered_Review) != 0:\n",
    "        negativeReviews.append(filtered_Review)\n",
    "    f.close() \n",
    "for file in files5:\n",
    "    f=open(file, 'r')  \n",
    "    try:\n",
    "        text = f.read()\n",
    "    except:\n",
    "        text.encode('utf-8').strip()\n",
    "    filtered_Review = removeStopWords(text)\n",
    "    if len(filtered_Review) != 0:\n",
    "        negativeReviews.append(filtered_Review)\n",
    "    f.close() \n",
    "\n",
    "print(len(negativeReviews))\n",
    "\n",
    "\n",
    "sentiment = []\n",
    "for i in range(541):\n",
    "    sentiment.append(0)\n",
    "\n",
    "dfNegative = pd.concat([pd.Series(sentiment), pd.Series(negativeReviews)], axis=1)\n",
    "dfNegative.columns = ['sentiment', 'text']\n",
    "dfNegative.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This homework annoying like every homeowork s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>This homework far hardest homework semester I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>HW1 relatively simple assignment complete lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>The first question felt overly tedious It dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>This homework difficult others Some questions...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0   This homework annoying like every homeowork s...\n",
       "1          0   This homework far hardest homework semester I...\n",
       "2          0   HW1 relatively simple assignment complete lot...\n",
       "3          0   The first question felt overly tedious It dif...\n",
       "4          0   This homework difficult others Some questions..."
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#processes all positive reviews into one dataframe\n",
    "\n",
    "path_FL16_pos= './dataset/hw_reviews/all_positive_reviews/fl16/*.txt'\n",
    "path_FL17_pos = './dataset/hw_reviews/all_positive_reviews/fl17/*.txt'\n",
    "path_FL18_pos = './dataset/hw_reviews/all_positive_reviews/fl18/*.txt'\n",
    "path_SP17_pos = './dataset/hw_reviews/all_positive_reviews/sp17/*.txt'\n",
    "path_SP18_pos = './dataset/hw_reviews/all_positive_reviews/sp18/*.txt'\n",
    "files = glob.glob(path_FL16_pos)\n",
    "files2 = glob.glob(path_FL17_pos)\n",
    "files3 = glob.glob(path_FL18_pos)\n",
    "files4 = glob.glob(path_SP17_pos)\n",
    "files5 = glob.glob(path_SP18_pos)\n",
    "positiveReviews = []\n",
    "for file in files:\n",
    "    f=open(file, encoding = 'utf-8')  \n",
    "    try:\n",
    "        text = f.read()\n",
    "    except:\n",
    "        text.encode('utf-8').strip()\n",
    "    filtered_Review = removeStopWords(text)\n",
    "    if len(filtered_Review) != 0:\n",
    "        positiveReviews.append(filtered_Review)\n",
    "    f.close() \n",
    "for file in files2:\n",
    "    f=open(file, encoding = 'utf-8')  \n",
    "    try:\n",
    "        text = f.read()\n",
    "    except:\n",
    "        text.encode('utf-8').strip()\n",
    "    filtered_Review = removeStopWords(text)\n",
    "    if len(filtered_Review) != 0:\n",
    "        positiveReviews.append(filtered_Review)\n",
    "    f.close() \n",
    "for file in files3:\n",
    "    f=open(file, encoding = 'utf-8')  \n",
    "    try:\n",
    "        text = f.read()\n",
    "    except:\n",
    "        text.encode('utf-8').strip()\n",
    "    filtered_Review = removeStopWords(text)\n",
    "    if len(filtered_Review) != 0:\n",
    "        positiveReviews.append(filtered_Review)\n",
    "    f.close() \n",
    "for file in files4:\n",
    "    f=open(file, encoding = 'utf-8')\n",
    "    try:\n",
    "        text = f.read()\n",
    "    except:\n",
    "        text.encode('utf-8').strip()\n",
    "    filtered_Review = removeStopWords(text)\n",
    "    if len(filtered_Review) != 0:\n",
    "        positiveReviews.append(filtered_Review)\n",
    "    f.close() \n",
    "for file in files5:\n",
    "    f=open(file, 'r')  \n",
    "    try:\n",
    "        text = f.read()\n",
    "    except:\n",
    "        text.encode('utf-8').strip()\n",
    "    filtered_Review = removeStopWords(text)\n",
    "    if len(filtered_Review) != 0:\n",
    "        positiveReviews.append(filtered_Review)\n",
    "    f.close() \n",
    "\n",
    "sentiment = []\n",
    "for i in range(len(positiveReviews)):\n",
    "    sentiment.append(4)\n",
    "dfPositive = pd.concat([pd.Series(sentiment), pd.Series(positiveReviews)], axis=1)\n",
    "dfPositive.columns = ['sentiment', 'text']\n",
    "\n",
    "frames = [dfNegative, dfPositive]\n",
    "dfHW = pd.concat(frames)\n",
    "\n",
    "dfHW.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting our dataset into training, testing set.\n",
    "split training into creating graph model, as well as creating the data mining model. \n",
    "split graph model into positive, negative, and neutral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain = dfHW.sample(frac = 0.8, random_state = 20)\n",
    "dfTest = dfHW.drop(dfTrain.index)\n",
    "\n",
    "dfTest.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dfGraphModel = dfTrain.sample(frac = 0.6, random_state = 20)\n",
    "dfDM = dfTrain.drop(dfGraphModel.index)\n",
    "dfDM.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "posSubject = dfGraphModel.loc[dfGraphModel['sentiment'] == 4]\n",
    "negSubject = dfGraphModel.loc[dfGraphModel['sentiment'] == 0]\n",
    "neutSubject = dfGraphModel.loc[dfGraphModel['sentiment'] == 2] #there is no neutral\n",
    "\n",
    "posGraphs = []\n",
    "negGraphs = []\n",
    "\n",
    "for row in posSubject.itertuples():\n",
    "    posG = createGraphFromTweet(row[1], 3)\n",
    "    posGraphs.append(posG)\n",
    "for row in negSubject.itertuples():\n",
    "    negG = createGraphFromTweet(row[1], 3)\n",
    "    negGraphs.append(negG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# writing functions to evaluate how each model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this function creates the word graph given the text in the tweet and the \n",
    "frame that is used to create the graph model. Can accept both an array of strings to \n",
    "treat as one long string, or a single string.'''\n",
    "\n",
    "def createGraphFromTweet(text, frame):\n",
    "    wordGraph = nx.DiGraph()\n",
    "    if type(text) == str:\n",
    "        createGraph(text, wordGraph, frame)\n",
    "    if type(text) == list:\n",
    "        for element in text:\n",
    "            createGraph(element, wordGraph, frame)\n",
    "    return wordGraph\n",
    "\n",
    "'''helper function- NOT FOR USE.'''\n",
    "def createGraph(text, wordGraph, frame):\n",
    "    if text:\n",
    "        text = re.sub(r'[^\\w\\s]', '', str(text))\n",
    "        text = text.split()\n",
    "        try:\n",
    "            if len(text) == 1:\n",
    "                wordGraph.add_edge(text[0], text[0], weight = 1)\n",
    "            for x in range(len(text) - frame):\n",
    "                for y in range(1, frame + 1):\n",
    "                    n1 = text[x]\n",
    "                    n2 = text[x+y]\n",
    "                    if wordGraph.has_edge(n1, n2):\n",
    "                        wordGraph[n1][n2]['weight'] = wordGraph[n1][n2]['weight'] + 1\n",
    "                    else:\n",
    "                        wordGraph.add_edge(n1, n2, weight = 1)\n",
    "            for x in reversed(range(1, frame + 1)):\n",
    "                for y in reversed(range(1, x)):\n",
    "                    n1 = text[len(text) - x]\n",
    "                    n2 = text[len(text) - y]\n",
    "                    if wordGraph.has_edge(n1, n2):\n",
    "                        wordGraph[n1][n2]['weight'] = wordGraph[n1][n2]['weight'] + 1\n",
    "                    else:\n",
    "                        wordGraph.add_edge(n1, n2, weight = 1)\n",
    "        except IndexError:\n",
    "            return createGraph(text, wordGraph, frame-1)\n",
    "        return wordGraph\n",
    "    else:\n",
    "        return wordGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "C:\\Users\\miche\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "'''extracting text from the dataframe'''\n",
    "posArrayText = posSubject.as_matrix(columns = posSubject.columns[1:]).flatten().tolist()\n",
    "negArrayText = negSubject.as_matrix(columns = negSubject.columns[1:]).flatten().tolist()\n",
    "\n",
    "'''putting text into one large graph'''\n",
    "posWordGraph = createGraphFromTweet(posArrayText, 10)\n",
    "negWordGraph = createGraphFromTweet(negArrayText, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GRAPH SIMILARITY FUNCTIONS'''\n",
    "\n",
    "'''counts the number of identical edges between two graphs, normalizes by the minimum number of nodes in either graph'''\n",
    "def edgeSimilarity(inputGraph, model):\n",
    "    count = 0\n",
    "    for edge in inputGraph.edges():\n",
    "        n1, n2 = edge\n",
    "        if model.has_edge(n1, n2): \n",
    "            count += 1\n",
    "    return count/min(len(inputGraph), len(model))\n",
    "\n",
    "'''returns maximum common subgraph of inputs'''\n",
    "def getMCS(graphModel, tweetGraph):\n",
    "    matching_graph = nx.DiGraph()\n",
    "    for n1, n2 in tweetGraph.edges():\n",
    "        if graphModel.has_edge(n1, n2):\n",
    "            matching_graph.add_edge(n1, n2, weight = tweetGraph[n1][n2]['weight'])\n",
    "    edgeList = list(matching_graph.edges())\n",
    "    edgeListCopy = edgeList[:]\n",
    "    nodeList = list(matching_graph.nodes())\n",
    "    visited = []\n",
    "    graphArray = []\n",
    "    while nodeList:\n",
    "        G = nx.DiGraph()\n",
    "        parentNode = nodeList.pop(0)\n",
    "        toRemove = []\n",
    "        for edge in edgeList:\n",
    "            n1, n2 = edge\n",
    "            if parentNode == n1:\n",
    "                if n2 in nodeList:\n",
    "                    G.add_edge(n1, n2)\n",
    "                    toRemove.append(edge)\n",
    "                    visited.append(n2)\n",
    "                else:\n",
    "                    for graph in graphArray:\n",
    "                        if n2 in graph.nodes():\n",
    "                            graph.add_edge(n1, n2)\n",
    "            edgeList = [e for e in edgeList if e not in toRemove]\n",
    "            toRemove = []\n",
    "            while visited:\n",
    "                node = visited.pop(0)\n",
    "                if node in nodeList:\n",
    "                    nodeList.remove(node)\n",
    "                for edge in edgeList:\n",
    "                    n1, n2 = edge\n",
    "                    if node == n1: \n",
    "                        G.add_edge(n1, n2)\n",
    "                        toRemove.append(edge)\n",
    "                        visited.append(n2)\n",
    "                edgeList = [e for e in edgeList if e not in toRemove]\n",
    "                toRemove = []\n",
    "        if len(G) != 0:\n",
    "            graphArray.append(G)\n",
    "\n",
    "    size = 0 \n",
    "    returnGraph = nx.DiGraph()\n",
    "    for graph in graphArray:\n",
    "        if len(graph) > size:\n",
    "            returnGraph = graph\n",
    "            size = len(graph)\n",
    "    return returnGraph\n",
    "        \n",
    "\n",
    "'''returns number of common nodes in MCS and model graph, normalized by minimum number of nodes'''\n",
    "def MCSNS(mcs_graph, graphModel, tweetGraph):\n",
    "    return len(mcs_graph)/min(len(graphModel),len(tweetGraph))\n",
    "\n",
    "'''returns number of common edges in MCS and model graph, normalized by minimum number of nodes'''\n",
    "def MCSUES(mcs_graph, graphModel, tweetGraph): \n",
    "    return len(mcs_graph.edges())/min(len(graphModel),len(tweetGraph))\n",
    "\n",
    "'''returns number of common edges in the MCS and model graph, taking direction into account,\n",
    "normalized by minimum number of nodes'''\n",
    "def MCSDES(mcs_graph, graphModel, tweetGraph): \n",
    "    count = 0\n",
    "    for e1,e2 in mcs_graph.edges():\n",
    "        if tweetGraph.has_edge(e1,e2) and graphModel.has_edge(e1,e2):\n",
    "            count+=1\n",
    "    return count/min(len(graphModel),len(tweetGraph))\n",
    "\n",
    "'''tf-idf helper function'''\n",
    "def calcTotalWords(model):\n",
    "    total = 0\n",
    "    for e1,e2 in model.edges(): #total number of edges = sum of weights\n",
    "        total += model[e1][e2]['weight']\n",
    "    return total\n",
    "\n",
    "'''tf-idf method for coming up with vectors, max edge value is taken as representative of\n",
    "the entire tweet model. Idea is that positive tweet should have a higher pos max than a neg max'''\n",
    "def tf_idf_max(tweetModel, posModel, negModel): #returns max score for each tweet model\n",
    "    maxPosVal = []\n",
    "    maxNegVal = []\n",
    "    totalPosEdges = calcTotalWords(posModel)\n",
    "    totalNegEdges = calcTotalWords(negModel)\n",
    "    for e1,e2 in tweetModel.edges():\n",
    "        if posModel.has_edge(e1, e2) and negModel.has_edge(e1, e2):      #both models have the edge\n",
    "            idf = math.log(1)                                #idf = 2 models, both have the edge -> 2/2 = 1\n",
    "            tf_pos = posModel[e1][e2]['weight'] / totalPosEdges\n",
    "            tf_neg = negModel[e1][e2]['weight'] / totalNegEdges\n",
    "            pos_score = idf * tf_pos\n",
    "            neg_score = idf * tf_neg\n",
    "            maxPosVal.append(pos_score)\n",
    "            maxNegVal.append(neg_score)\n",
    "        elif posModel.has_edge(e1, e2) or negModel.has_edge(e1, e2):          #only one of the models has the edge\n",
    "            idf = math.log(2)                              #idf: 2 models, only 1 has the edge -> 2/1 = 2\n",
    "            if posModel.has_edge(e1, e2):\n",
    "                tf_pos = posModel[e1][e2]['weight'] / totalPosEdges\n",
    "                pos_score = idf * tf_pos\n",
    "                maxPosVal.append(pos_score)\n",
    "            else:\n",
    "                tf_neg = negModel[e1][e2]['weight'] / totalNegEdges\n",
    "                neg_score = idf * tf_neg\n",
    "                maxNegVal.append(neg_score)\n",
    "    if len(maxPosVal) == 0:\n",
    "        maxPosVal.append(0)\n",
    "    if len(maxNegVal) == 0:\n",
    "        maxNegVal.append(0)\n",
    "    return max(maxPosVal), max(maxNegVal)\n",
    "\n",
    "\n",
    "'''tf-idf method for coming up with vectors, avg edge value is taken as representative of\n",
    "the entire tweet model.'''\n",
    "def tf_idf_avg(tweetModel, posModel, negModel): #returns average tfidf score for each model\n",
    "    totalPosEdges = calcTotalWords(posModel)\n",
    "    totalNegEdges = calcTotalWords(negModel)\n",
    "    negModelSum = 0\n",
    "    posModelSum = 0\n",
    "    totalPosEdgesTweet = 0\n",
    "    totalNegEdgesTweet = 0\n",
    "    for e1,e2 in tweetModel.edges():\n",
    "        if posModel.has_edge(e1, e2) and negModel.has_edge(e1, e2):      #both models have the edge\n",
    "            idf = math.log(1)                                #idf = 2 models, both have the edge -> 2/2 = 1\n",
    "            tf_pos = posModel[e1][e2]['weight'] / totalPosEdges\n",
    "            tf_neg = negModel[e1][e2]['weight'] / totalNegEdges\n",
    "            pos_score = idf * tf_pos\n",
    "            neg_score = idf * tf_neg\n",
    "            posModelSum += pos_score\n",
    "            negModelSum += neg_score\n",
    "            totalPosEdgesTweet += 1\n",
    "            totalNegEdgesTweet += 1\n",
    "        elif posModel.has_edge(e1, e2) or negModel.has_edge(e1, e2):          #only one of the models has the edge\n",
    "            idf = math.log(2)                              #idf: 2 models, only 1 has the edge -> 2/1 = 2\n",
    "            if posModel.has_edge(e1, e2):\n",
    "                tf_pos = posModel[e1][e2]['weight'] / totalPosEdges\n",
    "                pos_score = idf * tf_pos\n",
    "                posModelSum += pos_score\n",
    "                totalPosEdgesTweet += 1\n",
    "            else:\n",
    "                tf_neg = negModel[e1][e2]['weight'] / totalNegEdges\n",
    "                neg_score = idf * tf_neg\n",
    "                negModelSum += neg_score\n",
    "                totalNegEdgesTweet += 1\n",
    "\n",
    "    if totalPosEdgesTweet == 0:\n",
    "        average_tfidf_pos = 0\n",
    "    else:\n",
    "        average_tfidf_pos = posModelSum / totalPosEdgesTweet\n",
    "    if totalNegEdgesTweet == 0:\n",
    "        average_tfidf_neg = 0\n",
    "    else:\n",
    "        average_tfidf_neg = negModelSum / totalNegEdgesTweet\n",
    "    \n",
    "    return average_tfidf_pos, average_tfidf_neg\n",
    "\n",
    "'''weight * ((pos/neg)(pos + neg)). idea is that number of times a word shows up in a positive\n",
    "vs negative graph matters. eg, if a term shows up an equal number of times in a pos graph as a \n",
    "neg graph, then it's a neutral word. '''\n",
    "def tf_idf2(tweetModel, posModel, negModel): #harrison's method\n",
    "    allPosScore = []\n",
    "    allNegScore = []\n",
    "    for e1, e2 in tweetModel.edges():\n",
    "        weight_tweet = tweetModel[e1][e2]['weight']\n",
    "        if posModel.has_edge(e1, e2) and negModel.has_edge(e1, e2):\n",
    "            weight_pos = posModel[e1][e2]['weight']\n",
    "            weight_neg = negModel[e1][e2]['weight']\n",
    "            pos_score = weight_tweet * ((weight_pos/weight_neg)/(weight_pos + weight_neg))\n",
    "            neg_score = weight_tweet *((weight_neg/weight_pos)/(weight_pos + weight_neg))\n",
    "            allPosScore.append(pos_score)\n",
    "            allNegScore.append(neg_score)\n",
    "        elif posModel.has_edge(e1, e2) or negModel.has_edge(e1, e2):\n",
    "            if posModel.has_edge(e1, e2):\n",
    "                weight_pos = posModel[e1][e2]['weight']\n",
    "                weight_neg = 1\n",
    "                pos_score = weight_tweet * ((weight_pos/weight_neg)/(weight_pos + weight_neg))\n",
    "                neg_score = weight_tweet *((weight_neg/weight_pos)/(weight_pos + weight_neg))\n",
    "            else:\n",
    "                weight_pos = 1\n",
    "                weight_neg = negModel[e1][e2]['weight']\n",
    "                pos_score = weight_tweet * ((weight_pos/weight_neg)/(weight_pos + weight_neg))\n",
    "                neg_score = weight_tweet *((weight_neg/weight_pos)/(weight_pos + weight_neg))\n",
    "            allPosScore.append(pos_score)\n",
    "            allNegScore.append(neg_score)\n",
    "    if len(allPosScore) == 0:\n",
    "        allPosScore.append(0)\n",
    "    if len(allNegScore) == 0:\n",
    "        allNegScore.append(0)\n",
    "    return max(allPosScore), max(allNegScore)\n",
    "\n",
    "def calculateNumberOfGraphs(e1, e2, graph):\n",
    "    count = 0\n",
    "    for g in graph:\n",
    "        if g.has_edge(e1,e2):\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''generates the feature vectors and labels (posValue, negValue).\n",
    "   -dataframeDict is the dataframe above turned into a dictionary, with ID as \n",
    "   key and the rest of the information as value, stored as a nested dictionary\n",
    "   -posWordGraph/negWordGraph are our pos/neg graph models\n",
    "   -metric type refers to one of the above functions listed in the above cell block'''\n",
    "def generateWordGraphVectors(dataframeDict, posWordGraph, negWordGraph, metricType):\n",
    "    X = []\n",
    "    y = []\n",
    "    count = 0\n",
    "    for key, value in dataframeDict.items():\n",
    "        if value['text']:\n",
    "            y.append(value['sentiment'])\n",
    "            wordGraph = createGraphFromTweet(value['text'], 4)\n",
    "            posNegArray = []\n",
    "            if metricType == \"edge\":\n",
    "                posNegArray.append(edgeSimilarity(wordGraph, posWordGraph))\n",
    "                posNegArray.append(edgeSimilarity(wordGraph, negWordGraph))\n",
    "            elif metricType == \"MCSNS\":\n",
    "                mcs_graph = getMCS(posWordGraph, wordGraph)\n",
    "                posNegArray.append(MCSNS(mcs_graph, wordGraph, posWordGraph))\n",
    "                mcs_graph = getMCS(negWordGraph, wordGraph)\n",
    "                posNegArray.append(MCSNS(mcs_graph, wordGraph, negWordGraph))\n",
    "            elif metricType == \"MCSUES\":\n",
    "                mcs_graph = getMCS(posWordGraph, wordGraph)\n",
    "                posNegArray.append(MCSUES(mcs_graph, wordGraph, posWordGraph))\n",
    "                mcs_graph = getMCS(negWordGraph, wordGraph)\n",
    "                posNegArray.append(MCSUES(mcs_graph, wordGraph, negWordGraph))\n",
    "            elif metricType == \"MCSDES\":\n",
    "                mcs_graph = getMCS(posWordGraph, wordGraph)\n",
    "                posNegArray.append(MCSDES(mcs_graph, wordGraph, posWordGraph))\n",
    "                mcs_graph = getMCS(negWordGraph, wordGraph)\n",
    "                posNegArray.append(MCSDES(mcs_graph, wordGraph, negWordGraph))\n",
    "            elif metricType == \"TFIDF_max\":\n",
    "                pos_avg, neg_avg = tf_idf_max(wordGraph, posWordGraph, negWordGraph)\n",
    "                posNegArray.append(pos_avg)\n",
    "                posNegArray.append(neg_avg)\n",
    "            elif metricType == \"TFIDF_avg\":\n",
    "                pos_avg, neg_avg = tf_idf_avg(wordGraph, posWordGraph, negWordGraph)\n",
    "                posNegArray.append(pos_avg)\n",
    "                posNegArray.append(neg_avg)\n",
    "            elif metricType == \"TFIDF_2\":\n",
    "                pos_avg, neg_avg = tf_idf2(wordGraph, posWordGraph, negWordGraph)\n",
    "                posNegArray.append(pos_avg)\n",
    "                posNegArray.append(neg_avg)\n",
    "            elif metricType == \"TFIDF_3\":\n",
    "                pos_avg, neg_avg = tf_idf3(wordGraph, posWordGraph, negWordGraph)\n",
    "                posNegArray.append(pos_avg)\n",
    "                posNegArray.append(neg_avg)\n",
    "            X.append(posNegArray)\n",
    "            count += 1\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# here we actually start evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''turn our dataframe into dictionary format'''\n",
    "sentimentTweetDict = dfDM.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.8484848484848485\n",
      "SVM score: 0.898989898989899\n",
      "Logistic regression score: 0.9040404040404041\n",
      "Decision tree score: 0.7525252525252525\n"
     ]
    }
   ],
   "source": [
    "'''evaluate model, vectors generated by edgeSimilarity function. \n",
    "   Model evaluated using kNN, SVM, logistic regression, and decision tree.'''\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraph, negWordGraph, \"edge\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraph, negWordGraph, \"edge\")\n",
    "\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1f31fb19fd0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xPos = []\n",
    "yPos = []\n",
    "xNeg = []\n",
    "yNeg = []\n",
    "\n",
    "for index in range(len(y)):\n",
    "    if y[index] == 4:\n",
    "        yPos.append(X[index][0])\n",
    "        xPos.append(X[index][1])\n",
    "    if y[index] == 0:\n",
    "        yNeg.append(X[index][0])\n",
    "        xNeg.append(X[index][1])\n",
    "        \n",
    "plt.scatter(xPos, yPos, color = 'red')\n",
    "plt.scatter(xNeg, yNeg, color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.8686868686868687\n",
      "SVM score: 0.898989898989899\n",
      "Logistic regression score: 0.898989898989899\n",
      "Decision tree score: 0.7676767676767676\n"
     ]
    }
   ],
   "source": [
    "'''identical as above, except vectors generated by MCSNS'''\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraph, negWordGraph, \"MCSNS\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraph, negWordGraph, \"MCSNS\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.8636363636363636\n",
      "SVM score: 0.898989898989899\n",
      "Logistic regression score: 0.9040404040404041\n",
      "Decision tree score: 0.7323232323232324\n"
     ]
    }
   ],
   "source": [
    "'''identical as above, except vectors generated by MCSUES'''\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraph, negWordGraph, \"MCSUES\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraph, negWordGraph, \"MCSUES\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.8636363636363636\n",
      "SVM score: 0.898989898989899\n",
      "Logistic regression score: 0.9040404040404041\n",
      "Decision tree score: 0.7424242424242424\n"
     ]
    }
   ],
   "source": [
    "'''identical as above, except vectors generated by MCSDES'''\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraph, negWordGraph, \"MCSDES\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraph, negWordGraph, \"MCSDES\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.8888888888888888\n",
      "SVM score: 0.898989898989899\n",
      "Logistic regression score: 0.898989898989899\n",
      "Decision tree score: 0.7929292929292929\n"
     ]
    }
   ],
   "source": [
    "'''evaluate, same as above, but with new \"trimmed\" graphs, using MCSDES'''\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraphTrimmed, negWordGraphTrimmed, \"TFIDF_max\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraphTrimmed, negWordGraphTrimmed, \"TFIDF_max\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.8181818181818182\n",
      "SVM score: 0.898989898989899\n",
      "Logistic regression score: 0.898989898989899\n",
      "Decision tree score: 0.702020202020202\n"
     ]
    }
   ],
   "source": [
    "'''evaluate, same as above, but with new \"trimmed\" graphs, using MCSDES'''\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraphTrimmed, negWordGraphTrimmed, \"TFIDF_avg\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraphTrimmed, negWordGraphTrimmed, \"TFIDF_avg\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN score: 0.8383838383838383\n",
      "SVM score: 0.898989898989899\n",
      "Logistic regression score: 0.898989898989899\n",
      "Decision tree score: 0.7070707070707071\n"
     ]
    }
   ],
   "source": [
    "'''evaluate, same as above, but with new \"trimmed\" graphs, using MCSDES'''\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraphTrimmed, negWordGraphTrimmed, \"TFIDF_2\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraphTrimmed, negWordGraphTrimmed, \"TFIDF_2\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# here we start considering edge weights. \n",
    "weights = how many times the edge appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''remove edges that have under a certain weight, also remove isolated \n",
    "nodes after we run this function.'''\n",
    "def removeEdgesByWeight(graph, threshold):\n",
    "    returnGraph = graph.copy()\n",
    "    edgeCountDict = nx.get_edge_attributes(returnGraph, 'weight')\n",
    "    for key, value in edgeCountDict.items():\n",
    "        if value <= threshold:\n",
    "            returnGraph.remove_edge(*key)\n",
    "    emptyNodes = list(nx.isolates(returnGraph))\n",
    "    returnGraph.remove_nodes_from(emptyNodes)\n",
    "    return returnGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''trim edges'''\n",
    "posWordGraphTrimmed = removeEdgesByWeight(posWordGraph, 1)\n",
    "negWordGraphTrimmed = removeEdgesByWeight(negWordGraph, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''evaluate, same as above, but with new \"trimmed\" graphs, using edge similarity'''\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraphTrimmed, negWordGraphTrimmed, \"edge\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraphTrimmed, negWordGraphTrimmed, \"edge\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''evaluate, same as above, but with new \"trimmed\" graphs, using MCSNS'''\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraphTrimmed, negWordGraphTrimmed, \"MCSNS\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraphTrimmed, negWordGraphTrimmed, \"MCSNS\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''evaluate, same as above, but with new \"trimmed\" graphs, using MCSUES'''\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraphTrimmed, negWordGraphTrimmed, \"MCSUES\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraphTrimmed, negWordGraphTrimmed, \"MCSUES\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''evaluate, same as above, but with new \"trimmed\" graphs, using MCSDES'''\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraphTrimmed, negWordGraphTrimmed, \"MCSDES\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraphTrimmed, negWordGraphTrimmed, \"MCSDES\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''evaluate, same as above, but with new \"trimmed\" graphs, using TFIDF'''\n",
    "X, y = generateWordGraphVectors(sentimentTweetDict, posWordGraphTrimmed, negWordGraphTrimmed, \"TFIDF\")\n",
    "testDict = dfTest.to_dict(orient='index')\n",
    "Xtest, ytest = generateWordGraphVectors(testDict, posWordGraphTrimmed, negWordGraphTrimmed, \"TFIDF\")\n",
    "\n",
    "model = kNN(n_neighbors = 5)\n",
    "model.fit(X, y)\n",
    "print(\"kNN score: \" + str(model.score(Xtest, ytest)))\n",
    "\n",
    "classifier = svm.SVC(kernel = \"linear\")\n",
    "classifier.fit(X, y)\n",
    "print(\"SVM score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = logreg()\n",
    "classifier.fit(X, y)\n",
    "print(\"Logistic regression score: \" + str(classifier.score(Xtest, ytest)))\n",
    "\n",
    "classifier = dectree()\n",
    "classifier.fit(X, y)\n",
    "print(\"Decision tree score: \" + str(classifier.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xPos = []\n",
    "yPos = []\n",
    "xNeg = []\n",
    "yNeg = []\n",
    "\n",
    "for index in range(len(y)):\n",
    "    if y[index] == 4:\n",
    "        yPos.append(X[index][0])\n",
    "        xPos.append(X[index][1])\n",
    "    if y[index] == 0:\n",
    "        yNeg.append(X[index][0])\n",
    "        xNeg.append(X[index][1])\n",
    "        \n",
    "plt.scatter(xPos, yPos, color = 'red')\n",
    "plt.scatter(xNeg, yNeg, color = 'blue')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
